---
title: "Text Data : A Tidy Approch Book"
author: "Atiq"
format: html
editor: visual
---

# Loading Libraries

```{r}
library(tm)
library(NLP)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(syuzhet)
library(ggplot2)
library(stringr)
library(tidyverse)
library(tidytext)
library(topicmodels)
library(tidyr)
library(dplyr)
library(slam)
library(text)
library(glmnet)
library(caret)
library(e1071)
library(igraph)
library(ggraph)
library(visNetwork)
library(LDAvis)
library(textmineR)
library(stringi)
library(pdftools)
library(rvest)
library(caret)
library(rsample)

```

# Loading Data

```{r}
#loading files
folder <- "C:\\Users\\Pc\\OneDrive - Higher Education Commission\\text_data_analysis\\economic_updates_2023_24"
folder
#Reading Files from Folder
filelist <- list.files(path = folder)
filelist <- paste(folder, "\\" ,filelist, sep="")
typeof(filelist)
```

# Making corpus

```{r}
a <- lapply(filelist, FUN = readLines) # for readin line 
corpus <- lapply(a, FUN = paste, collapse= " ")

# Convert the corpus to a data frame
corpus_df <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = FALSE)

# Convert to a tibble
corpus_tbl <- tibble(line = 1:nrow(corpus_df), text = corpus_df$text)
```

# Tokenization And Cleaning

```{r}
# Tokenization
tidy_corpus <- corpus_tbl %>%
  unnest_tokens(word, text)

# Remove stop words
data("stop_words")
tidy_corpus <- tidy_corpus %>%
  anti_join(stop_words)

# Remove numbers and punctuation
tidy_corpus <- tidy_corpus %>%
  filter(!str_detect(word, "\\d")) %>%
  filter(!str_detect(word, "\\W"))
```

# Word Frequencies and Visualisation

```{r}
# Word frequency
word_freq <- tidy_corpus %>%
  count(word, sort = TRUE)

# Plotting word frequency
word_freq %>%
  filter(n > 20) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Word", y = "Frequency", title = "Word Frequency")
```

# Sentiment Analysis

```{r}
# Sentiment analysis using the Bing lexicon
sentiment_bing <- tidy_corpus %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)

# Plotting sentiment analysis
sentiment_bing %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  coord_flip() +
  labs(title = "Sentiment Analysis", y = "Contribution to sentiment", x = NULL)
```

# N-grams and Word Networks

```{r}
# Bigrams
bigrams <- corpus_tbl %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# Separate and filter stop words
bigrams_separated <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# Count bigrams
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

# Plotting bigrams
bigram_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n)) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1.5, hjust = 1.5) +
  theme_void()
```

# Topic Modeling

```{r}
# Create Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)

# Convert to matrix and then to tidy format
dtm_tidy <- tidy(dtm)

# Run LDA for topic modeling
lda <- LDA(dtm, k = 4, control = list(seed = 1234))

# Extract topics
topics <- tidy(lda, matrix = "beta")

# Top terms for each topic
top_terms <- topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# Plotting topics
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = "Term", y = "Beta", title = "Top Terms in Each Topic")

```

# Test and Train Data Practice

## 1. splitting the data into train (80%) and test (20%)

```{r}
set.seed(1234)
train_indices <- createDataPartition(corpus_tbl$line, p = 0.8, list = FALSE)
train_data <- corpus_tbl[train_indices, ]
test_data <- corpus_tbl[-train_indices, ]

# Verify the split
print(paste("Training set size:", nrow(train_data)))
print(paste("Test set size:", nrow(test_data)))
```

## Repeating the same Text data procedure

```{r}
# Tokenization
tidy_train <- train_data %>%
  unnest_tokens(word, text)

# Remove stop words
data("stop_words")
tidy_train <- tidy_train %>%
  anti_join(stop_words)

# Remove numbers and punctuation
tidy_train <- tidy_train %>%
  filter(!str_detect(word, "\\d")) %>%
  filter(!str_detect(word, "\\W"))

```

```{r}
# Word frequency
word_freq <- tidy_train %>%
  count(word, sort = TRUE)

# Plotting word frequency
word_freq %>%
  filter(n > 20) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Word", y = "Frequency", title = "Word Frequency")

```
